ğŸ§  IntelliDoc+: An NLP-Enhanced Multi-Document RAG System using LangChain & Deep Learning
ğŸ“Œ Overview
IntelliDoc+ is an advanced NLP + DL powered Retrieval-Augmented Generation (RAG) system that lets users upload multiple documents (PDFs) and ask natural language questions to retrieve precise, well-grounded answers from their content. It integrates semantic chunking, vector search, intent classification, LLM-based answer generation, and reranking with postprocessing â€” all wrapped in a modern Streamlit UI.

ğŸ“ Architecture Diagram
PDFs â†’ NLP Preprocessing â†’ Chunking â†’ Embeddings â†’ Vector Store
        â†‘                     â†“                     â†“
      Classifier       Query Understanding â†’ Retriever + Reranker
                                                 â†“
                                            LangChain + LLM
                                                 â†“
                                        Answer + Sources â†’ UI


ğŸš€ Features
ğŸ“„ Multi-PDF upload and processing

âœ‚ï¸ Semantic-aware chunking using NLP (spaCy / Sentence-BERT)

ğŸ¤– Embedding with OpenAI / HuggingFace models

ğŸ” Fast vector search using FAISS or ChromaDB

ğŸ§  Query classification and intent routing

ğŸ§¬ Reranking using transformer-based similarity models (SBERT, BGE)

ğŸ§¾ Answer generation using LLMs (OpenAI, Claude, Mistral, etc.)

ğŸ§¹ Postprocessing (NER highlighting, grammar fix)

ğŸ“Š Retrieval evaluation with Recall@k, latency, precision

ğŸŒ Streamlit front-end for interaction


ğŸ§° Tech Stack

| Component  | Tools / Models                                    |
| ---------- | ------------------------------------------------- |
| LLMs       | OpenAI GPT-3.5 / Claude / Phi-3 / Mistral         |
| Embeddings | `all-MiniLM`, `instructor-xl`, `text-embedding-3` |
| Vector DB  | FAISS / Chroma                                    |
| NLP        | spaCy, NLTK, Transformers                         |
| UI         | Streamlit                                         |
| Evaluation | Custom + `RAGAS` (optional)                       |

ğŸ“ Folder Structure
project_root/
â”œâ”€â”€ app.py                  # Streamlit UI
â”œâ”€â”€ main.py                 # Pipeline orchestrator
â”œâ”€â”€ loader.py               # PDF loader logic
â”œâ”€â”€ chunker.py              # NLP chunker (NER, POS, lemmatization)
â”œâ”€â”€ embedder.py             # Embedding generator
â”œâ”€â”€ vector_store.py         # Vector DB logic
â”œâ”€â”€ classifier.py           # DL-based query classifier
â”œâ”€â”€ reranker.py             # SBERT / BGE reranker
â”œâ”€â”€ rag_chain.py            # LangChain-based chain logic
â”œâ”€â”€ postprocessor.py        # NER-based and grammar postprocessing
â”œâ”€â”€ evaluation.py           # Recall@k, F1, latency, etc.
â”œâ”€â”€ nlp_utils.py            # Utility functions using spaCy/NLTK
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/            # Uploaded PDFs
â”‚   â””â”€â”€ preprocessed/       # Cleaned, chunked data
â”œâ”€â”€ vector_db/              # Serialized vector store
â””â”€â”€ assets/                 # Flowcharts, images, docs


âœ… How It Works
Upload PDFs

Text is extracted â†’ preprocessed â†’ semantically chunked

Chunks embedded and stored in FAISS

User types a question â†’ query classified

Retriever fetches relevant chunks

Reranker reorders by relevance

LLM generates the final answer

NER tags, grammar fix, and sources are added

Result shown in Streamlit UI


ğŸ§ª Evaluation
Recall@k: % of relevant chunks in top-k results

Latency: Response time breakdown (retriever vs LLM)

Manual QA: Evaluate factual correctness

Precision: Answers matching gold reference sets

ğŸ“Œ Future Extensions
 Add voice-based input via Whisper

 Replace OpenAI with local models (e.g., Mistral, Phi-3)

 Document summarization before embedding

 Metadata filters (e.g., by source, page, section)

